{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b867b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from env import load_uavs, load_tasks, initialize_targets\n",
    "from calculate import calculate_all_voyage_distance\n",
    "from runEnv import UAVEnv\n",
    "from model.dqn import DQN, ReplayBuffer\n",
    "from model.pso import PSO\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a847a2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练流程\n",
    "def train_dqn(\n",
    "    env,\n",
    "    episodes=500,\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    lr=1e-3,\n",
    "    eps_start=1.0,\n",
    "    eps_end=0.01,\n",
    "    eps_decay=0.995,\n",
    "):\n",
    "    state_dim = len(env.reset())\n",
    "    action_dim = len(env.uavs)\n",
    "    policy_net = DQN(state_dim, action_dim)\n",
    "    target_net = DQN(state_dim, action_dim)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "    buffer = ReplayBuffer(10000)\n",
    "    eps = eps_start\n",
    "\n",
    "    # 用于存储每集的总 reward\n",
    "    rewards_per_episode = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()\n",
    "        # 记录每次实验的数据，判断优化程度\n",
    "        total_reward = 0\n",
    "        total_success = 0\n",
    "        total_distance = 0\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            if random.random() < eps:\n",
    "                action = random.randrange(action_dim)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_vals = policy_net(torch.tensor(state).unsqueeze(0))\n",
    "                    action = q_vals.argmax().item()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if reward > 0:\n",
    "                total_success += 1\n",
    "            buffer.push(\n",
    "                state,\n",
    "                action,\n",
    "                reward,\n",
    "                next_state if next_state is not None else np.zeros_like(state),\n",
    "                done,\n",
    "            )\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if len(buffer) >= batch_size:\n",
    "                s, a, r, s2, d = buffer.sample(batch_size)\n",
    "                s = torch.tensor(s, dtype=torch.float32)\n",
    "                a = torch.tensor(a)\n",
    "                r = torch.tensor(r, dtype=torch.float32)\n",
    "                s2 = torch.tensor(s2, dtype=torch.float32)\n",
    "                d = torch.tensor(d, dtype=torch.float32)\n",
    "\n",
    "                q_pred = policy_net(s).gather(1, a.unsqueeze(1)).squeeze()\n",
    "                with torch.no_grad():\n",
    "                    q_next = target_net(s2).max(1)[0]\n",
    "                q_target = r + gamma * q_next * (1 - d)\n",
    "\n",
    "                loss = nn.functional.mse_loss(q_pred, q_target)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # 每集结束，记录 total_r\n",
    "        rewards_per_episode.append(total_reward)        \n",
    "        eps = max(eps_end, eps * eps_decay)\n",
    "        if ep % 10 == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "        total_distance = calculate_all_voyage_distance(env.uavs)\n",
    "        print(f\"Episode {ep} | Total Reward: {total_reward:.2f} | Total Distance: {total_distance:.2f} | \\\n",
    "Total Success : {total_success} | Epsilon: {eps:.3f}\")\n",
    "\n",
    "        # 每 50 轮绘制一次 reward 曲线\n",
    "        if (ep + 1) % 50 == 0:\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            plt.plot(range(1, ep+2), rewards_per_episode, marker='o')\n",
    "            plt.xlabel('Episode')\n",
    "            plt.ylabel('Total Reward')\n",
    "            plt.title(f'Total Reward up to Episode {ep+1}')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "    return policy_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d758ae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置数据目录\n",
    "uav_csv = \"data/uav.csv\"\n",
    "task_csv = \"data/task.csv\"\n",
    "uavs = load_uavs(uav_csv)\n",
    "target = initialize_targets(load_tasks(task_csv))\n",
    "env = UAVEnv(uavs, target)\n",
    "\n",
    "# model = train_dqn(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2174eea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10/100 - Best Fitness: 0.8927\n",
      "Iteration 20/100 - Best Fitness: 0.9188\n",
      "Iteration 30/100 - Best Fitness: 0.9197\n",
      "Iteration 40/100 - Best Fitness: 0.9208\n",
      "Iteration 50/100 - Best Fitness: 0.9208\n",
      "Iteration 60/100 - Best Fitness: 0.9208\n",
      "Iteration 70/100 - Best Fitness: 0.9208\n",
      "Iteration 80/100 - Best Fitness: 0.9208\n",
      "Iteration 90/100 - Best Fitness: 0.9208\n",
      "Iteration 100/100 - Best Fitness: 0.9208\n",
      "\n",
      "PSO Optimization Results:\n",
      "Total Voyage: 0.00\n",
      "Task Completion Rate: 100.00%\n",
      "Average Task-UAV Fitness: 0.8021\n",
      "Overall Fitness: 0.9208\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "运行PSO优化并返回结果\n",
    "\"\"\"\n",
    "pso = PSO(env, num_particles=30, max_iter=100)\n",
    "best_position, best_fitness = pso.optimize()\n",
    "metrics = pso.get_metrics()\n",
    "print(\"\\nPSO Optimization Results:\")\n",
    "print(f\"Total Voyage: {metrics['total_voyage']:.2f}\")\n",
    "print(f\"Task Completion Rate: {metrics['completion_rate']:.2%}\")\n",
    "print(f\"Average Task-UAV Fitness: {metrics['avg_fitness']:.4f}\")\n",
    "print(f\"Overall Fitness: {best_fitness:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
