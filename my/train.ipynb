{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b867b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from env import load_uavs, load_tasks, initialize_targets\n",
    "from runEnv import UAVEnv\n",
    "from model.dqn import DQN, ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a847a2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练流程\n",
    "def train_dqn(\n",
    "    env,\n",
    "    episodes=500,\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    lr=1e-3,\n",
    "    eps_start=1.0,\n",
    "    eps_end=0.01,\n",
    "    eps_decay=0.995,\n",
    "):\n",
    "    state_dim = len(env.reset())\n",
    "    action_dim = len(env.uavs)\n",
    "    policy_net = DQN(state_dim, action_dim)\n",
    "    target_net = DQN(state_dim, action_dim)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "    buffer = ReplayBuffer(10000)\n",
    "    eps = eps_start\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_r = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            if random.random() < eps:\n",
    "                action = random.randrange(action_dim)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_vals = policy_net(torch.tensor(state).unsqueeze(0))\n",
    "                    action = q_vals.argmax().item()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            buffer.push(\n",
    "                state,\n",
    "                action,\n",
    "                reward,\n",
    "                next_state if next_state is not None else np.zeros_like(state),\n",
    "                done,\n",
    "            )\n",
    "            state = next_state\n",
    "            total_r += reward\n",
    "\n",
    "            if len(buffer) >= batch_size:\n",
    "                s, a, r, s2, d = buffer.sample(batch_size)\n",
    "                s = torch.tensor(s, dtype=torch.float32)\n",
    "                a = torch.tensor(a)\n",
    "                r = torch.tensor(r, dtype=torch.float32)\n",
    "                s2 = torch.tensor(s2, dtype=torch.float32)\n",
    "                d = torch.tensor(d, dtype=torch.float32)\n",
    "\n",
    "                q_pred = policy_net(s).gather(1, a.unsqueeze(1)).squeeze()\n",
    "                with torch.no_grad():\n",
    "                    q_next = target_net(s2).max(1)[0]\n",
    "                q_target = r + gamma * q_next * (1 - d)\n",
    "\n",
    "                loss = nn.functional.mse_loss(q_pred, q_target)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        eps = max(eps_end, eps * eps_decay)\n",
    "        if ep % 10 == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "        print(f\"Episode {ep} | Total Reward: {total_r:.2f} | Epsilon: {eps:.3f}\")\n",
    "\n",
    "    return policy_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d758ae6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 | Total Reward: -9.93 | Epsilon: 0.995\n",
      "Episode 1 | Total Reward: -16.92 | Epsilon: 0.990\n",
      "Episode 2 | Total Reward: -21.07 | Epsilon: 0.985\n",
      "Episode 3 | Total Reward: -22.35 | Epsilon: 0.980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dnxkb\\AppData\\Local\\Temp\\ipykernel_35788\\1207239996.py:48: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  s2 = torch.tensor(s2, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4 | Total Reward: -25.52 | Epsilon: 0.975\n",
      "Episode 5 | Total Reward: -26.29 | Epsilon: 0.970\n",
      "Episode 6 | Total Reward: -26.85 | Epsilon: 0.966\n",
      "Episode 7 | Total Reward: -30.00 | Epsilon: 0.961\n",
      "Episode 8 | Total Reward: -30.00 | Epsilon: 0.956\n",
      "Episode 9 | Total Reward: -30.00 | Epsilon: 0.951\n",
      "Episode 10 | Total Reward: -30.00 | Epsilon: 0.946\n",
      "Episode 11 | Total Reward: -30.00 | Epsilon: 0.942\n",
      "Episode 12 | Total Reward: -30.00 | Epsilon: 0.937\n",
      "Episode 13 | Total Reward: -30.00 | Epsilon: 0.932\n",
      "Episode 14 | Total Reward: -30.00 | Epsilon: 0.928\n",
      "Episode 15 | Total Reward: -30.00 | Epsilon: 0.923\n",
      "Episode 16 | Total Reward: -30.00 | Epsilon: 0.918\n",
      "Episode 17 | Total Reward: -30.00 | Epsilon: 0.914\n",
      "Episode 18 | Total Reward: -30.00 | Epsilon: 0.909\n",
      "Episode 19 | Total Reward: -30.00 | Epsilon: 0.905\n",
      "Episode 20 | Total Reward: -30.00 | Epsilon: 0.900\n",
      "Episode 21 | Total Reward: -30.00 | Epsilon: 0.896\n",
      "Episode 22 | Total Reward: -30.00 | Epsilon: 0.891\n",
      "Episode 23 | Total Reward: -30.00 | Epsilon: 0.887\n",
      "Episode 24 | Total Reward: -30.00 | Epsilon: 0.882\n",
      "Episode 25 | Total Reward: -30.00 | Epsilon: 0.878\n",
      "Episode 26 | Total Reward: -30.00 | Epsilon: 0.873\n",
      "Episode 27 | Total Reward: -30.00 | Epsilon: 0.869\n",
      "Episode 28 | Total Reward: -30.00 | Epsilon: 0.865\n",
      "Episode 29 | Total Reward: -30.00 | Epsilon: 0.860\n",
      "Episode 30 | Total Reward: -30.00 | Epsilon: 0.856\n",
      "Episode 31 | Total Reward: -30.00 | Epsilon: 0.852\n",
      "Episode 32 | Total Reward: -30.00 | Epsilon: 0.848\n",
      "Episode 33 | Total Reward: -30.00 | Epsilon: 0.843\n",
      "Episode 34 | Total Reward: -30.00 | Epsilon: 0.839\n",
      "Episode 35 | Total Reward: -30.00 | Epsilon: 0.835\n",
      "Episode 36 | Total Reward: -30.00 | Epsilon: 0.831\n",
      "Episode 37 | Total Reward: -30.00 | Epsilon: 0.827\n",
      "Episode 38 | Total Reward: -30.00 | Epsilon: 0.822\n",
      "Episode 39 | Total Reward: -30.00 | Epsilon: 0.818\n",
      "Episode 40 | Total Reward: -30.00 | Epsilon: 0.814\n",
      "Episode 41 | Total Reward: -30.00 | Epsilon: 0.810\n",
      "Episode 42 | Total Reward: -30.00 | Epsilon: 0.806\n",
      "Episode 43 | Total Reward: -30.00 | Epsilon: 0.802\n",
      "Episode 44 | Total Reward: -30.00 | Epsilon: 0.798\n",
      "Episode 45 | Total Reward: -30.00 | Epsilon: 0.794\n",
      "Episode 46 | Total Reward: -30.00 | Epsilon: 0.790\n",
      "Episode 47 | Total Reward: -30.00 | Epsilon: 0.786\n",
      "Episode 48 | Total Reward: -30.00 | Epsilon: 0.782\n",
      "Episode 49 | Total Reward: -30.00 | Epsilon: 0.778\n",
      "Episode 50 | Total Reward: -30.00 | Epsilon: 0.774\n",
      "Episode 51 | Total Reward: -30.00 | Epsilon: 0.771\n",
      "Episode 52 | Total Reward: -30.00 | Epsilon: 0.767\n",
      "Episode 53 | Total Reward: -30.00 | Epsilon: 0.763\n",
      "Episode 54 | Total Reward: -30.00 | Epsilon: 0.759\n",
      "Episode 55 | Total Reward: -30.00 | Epsilon: 0.755\n",
      "Episode 56 | Total Reward: -30.00 | Epsilon: 0.751\n",
      "Episode 57 | Total Reward: -30.00 | Epsilon: 0.748\n",
      "Episode 58 | Total Reward: -30.00 | Epsilon: 0.744\n",
      "Episode 59 | Total Reward: -30.00 | Epsilon: 0.740\n",
      "Episode 60 | Total Reward: -30.00 | Epsilon: 0.737\n",
      "Episode 61 | Total Reward: -30.00 | Epsilon: 0.733\n",
      "Episode 62 | Total Reward: -30.00 | Epsilon: 0.729\n",
      "Episode 63 | Total Reward: -30.00 | Epsilon: 0.726\n",
      "Episode 64 | Total Reward: -30.00 | Epsilon: 0.722\n",
      "Episode 65 | Total Reward: -30.00 | Epsilon: 0.718\n",
      "Episode 66 | Total Reward: -30.00 | Epsilon: 0.715\n",
      "Episode 67 | Total Reward: -30.00 | Epsilon: 0.711\n",
      "Episode 68 | Total Reward: -30.00 | Epsilon: 0.708\n",
      "Episode 69 | Total Reward: -30.00 | Epsilon: 0.704\n",
      "Episode 70 | Total Reward: -30.00 | Epsilon: 0.701\n",
      "Episode 71 | Total Reward: -30.00 | Epsilon: 0.697\n",
      "Episode 72 | Total Reward: -30.00 | Epsilon: 0.694\n",
      "Episode 73 | Total Reward: -30.00 | Epsilon: 0.690\n",
      "Episode 74 | Total Reward: -30.00 | Epsilon: 0.687\n",
      "Episode 75 | Total Reward: -30.00 | Epsilon: 0.683\n",
      "Episode 76 | Total Reward: -30.00 | Epsilon: 0.680\n",
      "Episode 77 | Total Reward: -30.00 | Epsilon: 0.676\n",
      "Episode 78 | Total Reward: -30.00 | Epsilon: 0.673\n",
      "Episode 79 | Total Reward: -30.00 | Epsilon: 0.670\n",
      "Episode 80 | Total Reward: -30.00 | Epsilon: 0.666\n",
      "Episode 81 | Total Reward: -30.00 | Epsilon: 0.663\n",
      "Episode 82 | Total Reward: -30.00 | Epsilon: 0.660\n",
      "Episode 83 | Total Reward: -30.00 | Epsilon: 0.656\n",
      "Episode 84 | Total Reward: -30.00 | Epsilon: 0.653\n",
      "Episode 85 | Total Reward: -30.00 | Epsilon: 0.650\n",
      "Episode 86 | Total Reward: -30.00 | Epsilon: 0.647\n",
      "Episode 87 | Total Reward: -30.00 | Epsilon: 0.643\n",
      "Episode 88 | Total Reward: -30.00 | Epsilon: 0.640\n",
      "Episode 89 | Total Reward: -30.00 | Epsilon: 0.637\n",
      "Episode 90 | Total Reward: -30.00 | Epsilon: 0.634\n",
      "Episode 91 | Total Reward: -30.00 | Epsilon: 0.631\n",
      "Episode 92 | Total Reward: -30.00 | Epsilon: 0.627\n",
      "Episode 93 | Total Reward: -30.00 | Epsilon: 0.624\n",
      "Episode 94 | Total Reward: -30.00 | Epsilon: 0.621\n",
      "Episode 95 | Total Reward: -30.00 | Epsilon: 0.618\n",
      "Episode 96 | Total Reward: -30.00 | Epsilon: 0.615\n",
      "Episode 97 | Total Reward: -30.00 | Epsilon: 0.612\n",
      "Episode 98 | Total Reward: -30.00 | Epsilon: 0.609\n",
      "Episode 99 | Total Reward: -30.00 | Epsilon: 0.606\n",
      "Episode 100 | Total Reward: -30.00 | Epsilon: 0.603\n",
      "Episode 101 | Total Reward: -30.00 | Epsilon: 0.600\n",
      "Episode 102 | Total Reward: -30.00 | Epsilon: 0.597\n",
      "Episode 103 | Total Reward: -30.00 | Epsilon: 0.594\n",
      "Episode 104 | Total Reward: -30.00 | Epsilon: 0.591\n",
      "Episode 105 | Total Reward: -30.00 | Epsilon: 0.588\n",
      "Episode 106 | Total Reward: -30.00 | Epsilon: 0.585\n",
      "Episode 107 | Total Reward: -30.00 | Epsilon: 0.582\n",
      "Episode 108 | Total Reward: -30.00 | Epsilon: 0.579\n",
      "Episode 109 | Total Reward: -30.00 | Epsilon: 0.576\n",
      "Episode 110 | Total Reward: -30.00 | Epsilon: 0.573\n",
      "Episode 111 | Total Reward: -30.00 | Epsilon: 0.570\n",
      "Episode 112 | Total Reward: -30.00 | Epsilon: 0.568\n",
      "Episode 113 | Total Reward: -30.00 | Epsilon: 0.565\n",
      "Episode 114 | Total Reward: -30.00 | Epsilon: 0.562\n"
     ]
    }
   ],
   "source": [
    "# 设置数据目录\n",
    "uav_csv = \"data/uav.csv\"\n",
    "task_csv = \"data/task.csv\"\n",
    "\n",
    "env = UAVEnv(load_uavs(uav_csv), initialize_targets(load_tasks(task_csv)))\n",
    "\n",
    "model = train_dqn(env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
